{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d8fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf2934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    f = open(path, \"r\")\n",
    "    data = f.readlines()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06fa97ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61530\n",
      "12216\n",
      "12105\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "with open('1b_benchmark.train.tokens', 'r', encoding='utf-8') as f:\n",
    "    train_data = f.readlines()\n",
    "with open('1b_benchmark.dev.tokens', 'r', encoding='utf-8') as f:\n",
    "    dev_data = f.readlines()\n",
    "with open('1b_benchmark.test.tokens', 'r', encoding='utf-8') as f:\n",
    "    test_data = f.readlines()\n",
    "print(len(train_data))\n",
    "print(len(dev_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99cb5f",
   "metadata": {},
   "source": [
    "# Task 1- Programming N gram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf4307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_token_freqeuncy(data, cut_off):\n",
    "  processed_data = []\n",
    "  vocab = dict({\"UNK\" : 0})\n",
    "\n",
    "  # tokenize and add stop to sentences\n",
    "  for sent in data:\n",
    "    sent = sent.split()\n",
    "    sent = sent + [\"STOP\"]\n",
    "    processed_data.append(sent)\n",
    "\n",
    "  # make a dictionary of token frequencies\n",
    "  for sents in processed_data:\n",
    "    for word in sents:\n",
    "      if (word in vocab.keys()):\n",
    "        vocab[word] += 1\n",
    "      else:\n",
    "        vocab[word] = 1\n",
    "\n",
    "  # if there are words in the token freq dict that occur less than 3 times, add to UNK count instead\n",
    "  for val in vocab.values():\n",
    "    if val == cut_off:\n",
    "      vocab[\"UNK\"] += val\n",
    "\n",
    "  # remove the out of vocab words in the token freqeuncy dict\n",
    "  keys = list(vocab.keys())\n",
    "  for key in keys:\n",
    "    val = vocab[key]\n",
    "    if val < cut_off:\n",
    "      del vocab[key]\n",
    "  total_words = sum(vocab.values())\n",
    "  return vocab, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca490c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(freq_dict, total_words):\n",
    "  token_prob = dict({})\n",
    "  for key in freq_dict.keys():\n",
    "    token_prob[key] = freq_dict[key]/total_words\n",
    "  return token_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4dee1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, vocab_dict):\n",
    "  processed_data = []\n",
    "# tokenize and add start and stop to sentences\n",
    "  for sent in data:\n",
    "    sent = sent.split()\n",
    "    sent = [\"START\"] + sent + [\"STOP\"]\n",
    "    processed_data.append(sent)\n",
    "\n",
    "  # replace out of vocab words with UNK using the unigram frequency dict\n",
    "  final_data = []\n",
    "  for sent in processed_data:\n",
    "    for index, token in enumerate(sent):\n",
    "      if token not in vocab_dict.keys():\n",
    "        sent[index] = \"UNK\"\n",
    "    final_data.append(sent)\n",
    "  return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb09a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, token_prob):\n",
    "  total_words = 0\n",
    "  mle = 0\n",
    "  for sent in data:\n",
    "    for word in sent[1:]:\n",
    "      total_words += 1\n",
    "      if (word in token_prob.keys()):\n",
    "        mle += math.log2(token_prob[word])\n",
    "      else:\n",
    "        mle += math.log2(token_prob[\"UNK\"])\n",
    "  return total_words, mle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670725f3",
   "metadata": {},
   "source": [
    "Bigram Token Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d9c4d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_token_freqeuncy(processed_data):\n",
    "  bigram = dict({\"UNK\":0})\n",
    "\n",
    "  # make a dictionary of bigram token frequencies\n",
    "  for sent in processed_data:\n",
    "    for index, word in enumerate(sent[:-1]):\n",
    "      if ((word, sent[index + 1]) in bigram.keys()):\n",
    "        bigram[(word, sent[index + 1])] += 1\n",
    "      else:\n",
    "        bigram[(word, sent[index + 1])] = 1\n",
    "\n",
    "  return bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99090d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_bigram(bigram_token_freq,uni_token_freq):\n",
    "  bigram_prob = dict({})\n",
    "  for key in bigram_token_freq.keys():\n",
    "    if(key[0] == \"START\"):\n",
    "      bigram_prob[key] = bigram_token_freq[key] / uni_token_freq[\"STOP\"]\n",
    "    elif(key[0] in uni_token_freq.keys()):\n",
    "      bigram_prob[key] = bigram_token_freq[key] / uni_token_freq[key[0]]\n",
    "    else:\n",
    "      bigram_prob[key] = bigram_token_freq[key] / uni_token_freq[\"UNK\"]\n",
    "  return bigram_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c2b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_bigram(data, vocab_dict):\n",
    "  processed_data = []\n",
    "\n",
    "  # tokenize and add start and stop to sentences\n",
    "  for sent in data:\n",
    "    sent = sent.split()\n",
    "    sent = [\"START\"] + sent + [\"STOP\"]\n",
    "    processed_data.append(sent)\n",
    "\n",
    "  # replace out of vocab words with UNK using the unigram frequency dict\n",
    "  unk_data = []\n",
    "  for sent in processed_data:\n",
    "    for index, token in enumerate(sent):\n",
    "      if (token not in vocab_dict):\n",
    "        sent[index] = \"UNK\"\n",
    "    unk_data.append(sent)\n",
    "\n",
    "  final_data = []\n",
    "  for sent in unk_data:\n",
    "    bigrams = []\n",
    "    for index, token in enumerate(sent[:-1]):\n",
    "      bigrams.append((token, sent[index+1]))\n",
    "    final_data.append(bigrams)\n",
    "\n",
    "  return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09947b9a",
   "metadata": {},
   "source": [
    "Trigram Token Freqeuncies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a17ad85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tri_training(data, vocab_dict):\n",
    "  processed_data = []\n",
    "\n",
    "  # tokenize and add start and stop to sentences\n",
    "  for sent in data:\n",
    "    sent = sent.split()\n",
    "    sent = [\"START\", \"START\"] + sent + [\"STOP\"]\n",
    "    processed_data.append(sent)\n",
    "\n",
    "  # replace out of vocab words with UNK using the unigram frequency dict\n",
    "  final_data = []\n",
    "  for sent in processed_data:\n",
    "    for index, token in enumerate(sent):\n",
    "      if token not in vocab_dict.keys():\n",
    "        sent[index] = \"UNK\"\n",
    "    final_data.append(sent)\n",
    "  return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02dfd17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_token_freqeuncy(processed_data):\n",
    "  trigram = dict({\"UNK\":0})\n",
    "\n",
    "  # make a dictionary of trigram token frequencies\n",
    "  for sent in processed_data:\n",
    "    for index, word in enumerate(sent[:-2]):\n",
    "      if ((word, sent[index + 1], sent[index + 2]) in trigram.keys()):\n",
    "        trigram[(word, sent[index + 1], sent[index + 2])] += 1\n",
    "      else:\n",
    "        trigram[(word, sent[index + 1], sent[index + 2])] = 1\n",
    "  return trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61e05e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_trigram(trigram_token_freq,bigram_token_freq, unigram_token_freq):\n",
    "  trigram_prob = dict({})\n",
    "  for key in trigram_token_freq.keys():\n",
    "    if((key[0], key[1]) == (\"START\", \"START\")):\n",
    "      trigram_prob[key] = trigram_token_freq[key] / unigram_token_freq[\"STOP\"]\n",
    "    elif((key[0], key[1]) in bigram_token_freq.keys()):\n",
    "      trigram_prob[key] = trigram_token_freq[key] / bigram_token_freq[(key[0], key[1])]\n",
    "    else:\n",
    "      trigram_prob[key] = 0\n",
    "\n",
    "  return trigram_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13b14df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_trigram(data, vocab_dict):\n",
    "  processed_data = []\n",
    "\n",
    "  # tokenize and add start and stop to sentences\n",
    "  for sent in data:\n",
    "    sent = sent.split()\n",
    "    sent = [\"START\", \"START\"] + sent + [\"STOP\"]\n",
    "    processed_data.append(sent)\n",
    "\n",
    "  # replace out of vocab words with UNK using the unigram frequency dict\n",
    "  unk_data = []\n",
    "  for sent in processed_data:\n",
    "    for index, token in enumerate(sent):\n",
    "      if (token not in vocab_dict):\n",
    "        sent[index] = \"UNK\"\n",
    "    unk_data.append(sent)\n",
    "\n",
    "  final_data = []\n",
    "  for sent in unk_data:\n",
    "    trigrams = []\n",
    "    for index, token in enumerate(sent[:-2]):\n",
    "      trigrams.append((token, sent[index + 1], sent[index + 2]))\n",
    "    final_data.append(trigrams)\n",
    "\n",
    "  return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89530e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_predict(data, token_prob):\n",
    "  total_words = 0\n",
    "  mle = 0\n",
    "  for sent in data:\n",
    "    for bigram in sent:\n",
    "      total_words += 1\n",
    "      if (bigram in token_prob.keys()):\n",
    "        mle += math.log2(token_prob[bigram])\n",
    "      else:\n",
    "        mle = -math.inf\n",
    "  return total_words, mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f266b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_predict(data, token_prob):\n",
    "  total_words = 0\n",
    "  mle = 0\n",
    "  for sent in data:\n",
    "    for trigram in sent:\n",
    "      total_words += 1\n",
    "      if (trigram in token_prob.keys()):\n",
    "        mle += math.log2(token_prob[trigram])\n",
    "      else:\n",
    "        mle = -math.inf\n",
    "  return total_words, mle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa0679",
   "metadata": {},
   "source": [
    "Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ace7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(total_words, mle):\n",
    "  perp = 2 ** ((-1 / total_words) * mle)\n",
    "  return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8e67787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1574164 61530\n"
     ]
    }
   ],
   "source": [
    "# train model ungirams\n",
    "unigram_vocab_dict, total_words = unigram_token_freqeuncy(train_data, 3)\n",
    "print(total_words, len(train_data))\n",
    "unigram_token_prob = prob(unigram_vocab_dict, total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cd7b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model bigrams\n",
    "processed_train = process_data(train_data, unigram_vocab_dict)\n",
    "bigram_vocab_dict = bigram_token_freqeuncy(processed_train)\n",
    "bigram_token_prob = prob_bigram(bigram_vocab_dict, unigram_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "305deb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model trigrams\n",
    "processed_train_tri = process_tri_training(train_data, unigram_vocab_dict)\n",
    "trigram_vocab_dict = trigram_token_freqeuncy(processed_train_tri)\n",
    "trigram_token_prob = prob_trigram(trigram_vocab_dict, bigram_vocab_dict, unigram_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f797569f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram perplexity for train 1001.7500328769861\n",
      "Bigram perplexity for train 72.87810137763663\n",
      "Trigram perplexity for train 7.873059029042192\n"
     ]
    }
   ],
   "source": [
    "# predict train unigram\n",
    "total_unigrams_train, unigram_mle_train = predict(processed_train, unigram_token_prob)\n",
    "unigram_perplexity_train = perplexity(total_unigrams_train, unigram_mle_train)\n",
    "print(\"Unigram perplexity for train\", unigram_perplexity_train)\n",
    "\n",
    "# predict train bigrams\n",
    "processed_train_bigram = process_data_bigram(train_data, unigram_vocab_dict)\n",
    "total_bigrams_train, bigram_mle_train = bigram_predict(processed_train_bigram, bigram_token_prob)\n",
    "bigram_perplexity_train = perplexity(total_bigrams_train, bigram_mle_train)\n",
    "print(\"Bigram perplexity for train\", bigram_perplexity_train)\n",
    "\n",
    "# predict train trigrams\n",
    "processed_train_trigram = process_data_trigram(train_data, unigram_vocab_dict)\n",
    "total_trigrams_train, trigram_mle_train = trigram_predict(processed_train_trigram, trigram_token_prob)\n",
    "trigram_perplexity_train = perplexity(total_trigrams_train, trigram_mle_train)\n",
    "print(\"Trigram perplexity for train\", trigram_perplexity_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec2e0b8",
   "metadata": {},
   "source": [
    "On Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec8def57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram perplexity for dev 928.7011934138095\n"
     ]
    }
   ],
   "source": [
    "# predict dev set unigram\n",
    "processed_dev = process_data(dev_data, unigram_vocab_dict)\n",
    "total_unigrams_dev, unigram_mle_dev = predict(processed_dev, unigram_token_prob)\n",
    "unigram_perplexity_dev = perplexity(total_unigrams_dev, unigram_mle_dev)\n",
    "print(\"Unigram perplexity for dev\", unigram_perplexity_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "368a6475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram perplexity for dev inf\n"
     ]
    }
   ],
   "source": [
    "# predict dev set bigrams\n",
    "processed_dev_bigram = process_data_bigram(dev_data, unigram_vocab_dict)\n",
    "total_bigrams_dev, bigram_mle_dev = bigram_predict(processed_dev_bigram, bigram_token_prob)\n",
    "bigram_perplexity_dev = perplexity(total_bigrams_dev, bigram_mle_dev)\n",
    "print(\"Bigram perplexity for dev\", bigram_perplexity_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d1298db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram perplexity for dev inf\n"
     ]
    }
   ],
   "source": [
    "# predict dev set trigrams\n",
    "processed_dev_trigram = process_data_trigram(dev_data, unigram_vocab_dict)\n",
    "total_trigrams_dev, trigram_mle_dev = trigram_predict(processed_dev_trigram, trigram_token_prob)\n",
    "trigram_perplexity_dev = perplexity(total_trigrams_dev, trigram_mle_dev)\n",
    "print(\"Trigram perplexity for dev\", trigram_perplexity_dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ef965",
   "metadata": {},
   "source": [
    "On Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68c92e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram perplexity for test 932.0903293095981\n"
     ]
    }
   ],
   "source": [
    "# predict test unigram\n",
    "processed_test = process_data(test_data, unigram_vocab_dict)\n",
    "total_unigrams_test, unigram_mle_test = predict(processed_test, unigram_token_prob)\n",
    "unigram_perplexity_test = perplexity(total_unigrams_test, unigram_mle_test)\n",
    "print(\"Unigram perplexity for test\", unigram_perplexity_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "952c6cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram perplexity for test inf\n"
     ]
    }
   ],
   "source": [
    "# predict test bigrams\n",
    "processed_test_bigram = process_data_bigram(test_data, unigram_vocab_dict)\n",
    "total_bigrams_test, bigram_mle_test = bigram_predict(processed_test_bigram, bigram_token_prob)\n",
    "bigram_perplexity_test = perplexity(total_bigrams_test, bigram_mle_test)\n",
    "print(\"Bigram perplexity for test\", bigram_perplexity_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2905997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram perplexity for dev inf\n"
     ]
    }
   ],
   "source": [
    "# predict test trigrams\n",
    "processed_test_trigram = process_data_trigram(test_data, unigram_vocab_dict)\n",
    "total_trigrams_test, trigram_mle_test = trigram_predict(processed_test_trigram, trigram_token_prob)\n",
    "trigram_perplexity_test = perplexity(total_trigrams_test, trigram_mle_test)\n",
    "print(\"Trigram perplexity for dev\", trigram_perplexity_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88972712",
   "metadata": {},
   "source": [
    "# Task 2 - LINEAR INTERPOLATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f75f9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(data, unigram_vocab_dict, bigram_vocab_dict, trigram_vocab_dict, unigram_token_prob, bigram_token_prob, trigram_token_prob, lambda1, lambda2, lambda3):\n",
    "  processed_data_uni = process_data(data, unigram_vocab_dict)\n",
    "  processed_data_bi = process_data_bigram(data, unigram_vocab_dict)\n",
    "  processed_data_tri = process_data_trigram(data, unigram_vocab_dict)\n",
    "\n",
    "  linear_interpolation_mle = 0\n",
    "  total_words = 0\n",
    "\n",
    "  for i in range(0, len(processed_data_uni)):\n",
    "    for j in range (0, len(processed_data_uni[i])-1):\n",
    "      total_words += 1\n",
    "\n",
    "      if(processed_data_uni[i][j+1] in unigram_token_prob.keys()):\n",
    "        uni_prob = unigram_token_prob[processed_data_uni[i][j+1]]\n",
    "      else:\n",
    "        uni_prob = 0\n",
    "      if(processed_data_bi[i][j] in bigram_token_prob.keys()):\n",
    "        bi_prob = bigram_token_prob[processed_data_bi[i][j]]\n",
    "      else:\n",
    "        bi_prob = 0\n",
    "      if(processed_data_tri[i][j] in trigram_token_prob.keys()):\n",
    "        tri_prob = trigram_token_prob[processed_data_tri[i][j]]\n",
    "      else:\n",
    "        tri_prob = 0\n",
    "      linear_interpolation_mle += math.log2((uni_prob * lambda1) + (bi_prob * lambda2) + (tri_prob * lambda3))\n",
    "  return linear_interpolation_mle, total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47dd876",
   "metadata": {},
   "source": [
    "* λ1 = 0.3, λ2 = 0.3, λ3 = 0.4\n",
    "λ1 = 0.4, λ2 = 0.3, λ3 = 0.2\n",
    "λ1 = 0.2, λ2 = 0.3, λ3 = 0.4\n",
    "λ1 = 0.3, λ2 = 0.2, λ3 = 0.4\n",
    "λ1 = 0.4, λ2 = 0.4, λ3 = 0.2\n",
    "λ1 = 0.5, λ2 = 0.3, λ3 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfe44842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of train with lambda values 0.4, 0.3, 0.2:  25.151237165008716\n",
      "Perplexity of dev with lambda values 0.4, 0.3, 0.2:    297.2434258749143 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model 1\n",
    "#Perplexity of train with lambda values λ1 = 0.4, λ2 = 0.3, λ3 = 0.2\n",
    "train_1, total_words = linear_interpolation(train_data, unigram_vocab_dict, bigram_vocab_dict, trigram_vocab_dict, unigram_token_prob, bigram_token_prob, trigram_token_prob, 0.4, 0.3, 0.2)\n",
    "train_perp_1 = perplexity(total_words, train_1)\n",
    "print(\"Perplexity of train with lambda values 0.4, 0.3, 0.2: \", train_perp_1)\n",
    "\n",
    "#Perplexity of dev with lambda values λ1 = 0.4, λ2 = 0.3, λ3 = 0.2\n",
    "dev_1, total_words1 = linear_interpolation(dev_data, unigram_vocab_dict, bigram_vocab_dict, trigram_vocab_dict, unigram_token_prob, bigram_token_prob, trigram_token_prob, 0.4, 0.3, 0.2)\n",
    "dev_perp_1 = perplexity(total_words1, dev_1)\n",
    "print(\"Perplexity of dev with lambda values 0.4, 0.3, 0.2:   \", dev_perp_1, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c73f1eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of train with lambda values 0.2, 0.3, 0.4:  15.296423833195202\n",
      "Perplexity of dev with lambda values 0.2, 0.3, 0.4:    315.343570028335 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model 2\n",
    "#Perplexity of train with lambda values λ1 = 0.2, λ2 = 0.3, λ3 = 0.4\n",
    "train_1, total_words = linear_interpolation(train_data, unigram_vocab_dict, bigram_vocab_dict, trigram_vocab_dict, unigram_token_prob, bigram_token_prob, trigram_token_prob, 0.2, 0.3, 0.4)\n",
    "train_perp_1 = perplexity(total_words, train_1)\n",
    "print(\"Perplexity of train with lambda values 0.2, 0.3, 0.4: \", train_perp_1)\n",
    "\n",
    "#Perplexity of dev with lambda values λ1 = 0.4, λ2 = 0.3, λ3 = 0.2\n",
    "dev_1, total_words1 = linear_interpolation(dev_data, unigram_vocab_dict, bigram_vocab_dict, trigram_vocab_dict, unigram_token_prob, bigram_token_prob, trigram_token_prob, 0.2, 0.3, 0.4)\n",
    "dev_perp_1 = perplexity(total_words1, dev_1)\n",
    "print(\"Perplexity of dev with lambda values 0.2, 0.3, 0.4:   \", dev_perp_1, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b1efa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of train with lambda values 0.4, 0.3, 0.2:  16.07853285728428\n",
      "Perplexity of dev with lambda values 0.4, 0.3, 0.2:    327.73955214115443 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model 3\n",
    "#Perplexity of train with lambda values λ1 = 0.3, λ2 = 0.2, λ3 = 0.4\n",
    "train_1, total_words = linear_interpolation(train_data, unigram_vocab_dict, bigram_vocab_dict, trigram_vocab_dict, unigram_token_prob, bigram_token_prob, trigram_token_prob, 0.3, 0.2, 0.4)\n",
    "train_perp_1 = perplexity(total_words, train_1)\n",
    "print(\"Perplexity of train with lambda values 0.4, 0.3, 0.2: \", train_perp_1)\n",
    "\n",
    "#Perplexity of dev with lambda values λ1 = 0.3, λ2 = 0.2, λ3 = 0.4\n",
    "dev_1, total_words1 = linear_interpolation(dev_data, unigram_vocab_dict, bigram_vocab_dict, trigram_vocab_dict, unigram_token_prob, bigram_token_prob, trigram_token_prob, 0.3, 0.2, 0.4)\n",
    "dev_perp_1 = perplexity(total_words1, dev_1)\n",
    "print(\"Perplexity of dev with lambda values 0.4, 0.3, 0.2:   \", dev_perp_1, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01ff5489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of train with lambda values 0.4, 0.4, 0.2:  23.286791883635704\n",
      "Perplexity of dev with lambda values 0.4, 0.4, 0.2:    259.4630274937098 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model 4\n",
    "#Perplexity of train with lambda values λ1 = 0.4, λ2 = 0.4, λ3 = 0.2\n",
    "train_1, total_words = linear_interpolation(train_data, unigram_vocab_dict, bigram_vocab_dict, trigram_vocab_dict, unigram_token_prob, bigram_token_prob, trigram_token_prob, 0.4, 0.4, 0.2)\n",
    "train_perp_1 = perplexity(total_words, train_1)\n",
    "print(\"Perplexity of train with lambda values 0.4, 0.4, 0.2: \", train_perp_1)\n",
    "\n",
    "#Perplexity of dev with lambda values λ1 = 0.4, λ2 = 0.4, λ3 = 0.2\n",
    "dev_1, total_words1 = linear_interpolation(dev_data, unigram_vocab_dict, bigram_vocab_dict, trigram_vocab_dict, unigram_token_prob, bigram_token_prob, trigram_token_prob, 0.4, 0.4, 0.2)\n",
    "dev_perp_1 = perplexity(total_words1, dev_1)\n",
    "print(\"Perplexity of dev with lambda values 0.4, 0.4, 0.2:   \", dev_perp_1, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10c171e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of train with lambda values 0.5, 0.3, 0.2:  24.81205153217311\n",
      "Perplexity of dev with lambda values 0.5, 0.3, 0.2:    274.43782125162784 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model 5\n",
    "#Perplexity of train with lambda values λ1 = 0.5, λ2 = 0.3, λ3 = 0.2\n",
    "train_1, total_words = linear_interpolation(train_data, unigram_vocab_dict, bigram_vocab_dict, trigram_vocab_dict, unigram_token_prob, bigram_token_prob, trigram_token_prob, 0.5, 0.3, 0.2)\n",
    "train_perp_1 = perplexity(total_words, train_1)\n",
    "print(\"Perplexity of train with lambda values 0.5, 0.3, 0.2: \", train_perp_1)\n",
    "\n",
    "#Perplexity of dev with lambda values λ1 = 0.4, λ2 = 0.4, λ3 = 0.2\n",
    "dev_1, total_words1 = linear_interpolation(dev_data, unigram_vocab_dict, bigram_vocab_dict, trigram_vocab_dict, unigram_token_prob, bigram_token_prob, trigram_token_prob, 0.5, 0.3, 0.2)\n",
    "dev_perp_1 = perplexity(total_words1, dev_1)\n",
    "print(\"Perplexity of dev with lambda values 0.5, 0.3, 0.2:   \", dev_perp_1, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a41feab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of train with lambda values 0.3, 0.3, 0.4:  15.13822057991001\n",
      "Perplexity of dev with lambda values 0.3, 0.3, 0.4:    278.8498077733136 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Report the training and development perplexity for the values λ1 = 0.3, λ2 = 0.3, λ3 = 0.4.\n",
    "#Perplexity of train with lambda values λ1 = 0.3, λ2 = 0.3, λ3 = 0.4\n",
    "train_1, total_words = linear_interpolation(train_data, unigram_vocab_dict, bigram_vocab_dict, trigram_vocab_dict, unigram_token_prob, bigram_token_prob, trigram_token_prob, 0.3, 0.3, 0.4)\n",
    "train_perp_1 = perplexity(total_words, train_1)\n",
    "print(\"Perplexity of train with lambda values 0.3, 0.3, 0.4: \", train_perp_1)\n",
    "\n",
    "#Perplexity of dev with lambda values λ1 = 0.4, λ2 = 0.4, λ3 = 0.2\n",
    "dev_1, total_words1 = linear_interpolation(dev_data, unigram_vocab_dict, bigram_vocab_dict, trigram_vocab_dict, unigram_token_prob, bigram_token_prob, trigram_token_prob, 0.3, 0.3, 0.4)\n",
    "dev_perp_1 = perplexity(total_words1, dev_1)\n",
    "print(\"Perplexity of dev with lambda values 0.3, 0.3, 0.4:   \", dev_perp_1, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c3118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
